{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import Concatenate, Input, Dense\n",
    "from keras.layers import merge\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_EPISODES = 400\n",
    "MAX_STEPS = 10000\n",
    "MAX_BUFFER = 40000\n",
    "MAX_TOTAL_REWARD = 500\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TAU = 0.1\n",
    "LAYER_1 = 128\n",
    "LAYER_2 = 64\n",
    "ALPHA = 0.0000001\n",
    "GAMMA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.buffer, size)\n",
    "        random_index_list = np.random.choice(np.arange(len(self.buffer)), size = size, replace = False)\n",
    "        sublist =  [self.buffer[index] for index in random_index_list]\n",
    "        states, actions, rewards, next_states, done = zip(*sublist)\n",
    "        return states, actions, rewards, next_states, done\n",
    "\n",
    "    def remember(self, experience):\n",
    "        self.buffer.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(LAYER_1, activation = 'relu', input_shape = (state_dim, ), kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(action_dim, activation = 'tanh', kernel_initializer='he_uniform'))\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss=self.newLoss)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model.predict(np.array(state))\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))\n",
    "\n",
    "    def train(self, state, errors, batch_size):\n",
    "        self.model.fit( state, errors, batch_size=batch_size, verbose = 0)\n",
    "\n",
    "    def newLoss(self, xPred,yPred):\n",
    "        return K.sum(-1*yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.first_input = Input((state_dim, ))\n",
    "        self.first_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.first_input)\n",
    "\n",
    "        self.second_input = Input((action_dim, ))\n",
    "        self.second_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.second_input)\n",
    "\n",
    "        self.merged = Concatenate(axis = 1)([self.first_dense, self.second_dense])\n",
    "        self.output_layer = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.merged)\n",
    "        self.output_layer_2 = Dense(1, kernel_initializer='he_uniform')(self.output_layer)\n",
    "        \n",
    "        self.model = Model(inputs=[self.first_input, self.second_input], outputs=self.output_layer_2)\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss='mse')\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.model.predict([state, action])\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, state_dim, action_dim, memory, load):\n",
    "        self.memory = memory\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(action_dim)\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.target_actor = Actor(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "        if load != 0:\n",
    "            self.load_models(load) #load the model\n",
    "\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "\n",
    "    def get_action(self, state, train):\n",
    "        if train:\n",
    "            action = self.actor.forward([state])\n",
    "            noise = np.float32(self.noise.sample())\n",
    "            return action + noise\n",
    "        action = self.actor.forward([state])\n",
    "        return action\n",
    "\n",
    "    def optimize(self):\n",
    "        state,action,reward,next_state, done = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        next_state = np.array(next_state)\n",
    "        done = 1-np.array(done)\n",
    "        \n",
    "        next_action = self.target_actor.forward(next_state)\n",
    "        target = reward[:,np.newaxis] + GAMMA* self.target_critic.forward(next_state, next_action)*done[:, np.newaxis]\n",
    "        self.critic.model.fit([state,action], target, batch_size=BATCH_SIZE, verbose = 0)\n",
    "\n",
    "        action = self.actor.forward(state)\n",
    "        self.actor.train(state, self.critic.forward(state, action), BATCH_SIZE)\n",
    "        \n",
    "        self.net_update(self.target_actor, self.actor, False)\n",
    "        self.net_update(self.target_critic, self.critic, False)\n",
    "\n",
    "    def net_update(self,target, source, hard):\n",
    "        degree = 1\n",
    "        if not hard: degree = TAU\n",
    "        target.update(source, degree)\n",
    "\n",
    "    def save_models(self, episode):\n",
    "        self.actor.model.save_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.save_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "\n",
    "    def load_models(self, episode):\n",
    "        self.actor.model.load_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.load_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "        print('Models loaded succesfully')\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, action_dim):\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = 0.15\n",
    "        self.sigma = 0.02\n",
    "        self.dx = np.zeros(self.action_dim)\n",
    "\n",
    "    def sample(self):\n",
    "        self.dx = self.dx + self.theta * (-self.dx) + self.sigma * np.random.randn(len(self.dx))\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\n",
      " Testing agent got a reward of : -495.40696960932246\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "\n",
      " Testing agent got a reward of : -744.121453729284\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "10\n",
      "\n",
      " Testing agent got a reward of : -781.8956276934927\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "15\n",
      "\n",
      " Testing agent got a reward of : -920.1599047950518\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0menv_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mtest_interwal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mmax_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36menv_run\u001b[1;34m(env, episode, trainer, memory, train)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ef97cb0554cd>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-edfcaa4497a2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def env_run(env, episode, trainer, memory, train):\n",
    "    state = env.reset()\n",
    "    epoch_reward = 0\n",
    "    print(episode)\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        if not train:\n",
    "            env.render()\n",
    "        action = trainer.get_action(state, train)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        epoch_reward +=reward\n",
    "        if train:\n",
    "            if done:\n",
    "                break\n",
    "            memory.remember((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            trainer.optimize()\n",
    "        else:\n",
    "            if done:\n",
    "                env.close()\n",
    "                print(\"\\n Testing agent got a reward of :\",epoch_reward)\n",
    "                break\n",
    "        state = next_state\n",
    "    gc.collect()\n",
    "    if episode%100 == 1:\n",
    "        trainer.save_models(episode)\n",
    "    return epoch_reward\n",
    "\n",
    "def prepopulate_memory(memory, env):\n",
    "    state = env.reset()\n",
    "    for _ in range(MAX_BUFFER):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.remember((state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "def main():\n",
    "\n",
    "    training = 1\n",
    "    test_interwal = 5\n",
    "    load = 0\n",
    "\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    memory = None\n",
    "\n",
    "    if training == 1:\n",
    "        memory = Memory(MAX_BUFFER)\n",
    "        prepopulate_memory(memory, env)\n",
    "    rewards = []\n",
    "    max_reward = 0\n",
    "\n",
    "    trainer = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0], memory, load)\n",
    "\n",
    "    for episode in np.arange(MAX_EPISODES):\n",
    "        if training == 1:\n",
    "            env_run(env, episode, trainer, memory, True)\n",
    "        if episode%test_interwal == 0:\n",
    "            max_reward += env_run(env, episode,trainer, None, False)\n",
    "            rewards.append(max_reward/((episode/test_interwal)+1))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
