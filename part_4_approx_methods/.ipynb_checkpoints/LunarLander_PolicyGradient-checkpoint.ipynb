{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import Concatenate, Input, Dense\n",
    "from keras.layers import merge\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_EPISODES = 400\n",
    "MAX_STEPS = 10000\n",
    "MAX_BUFFER = 40000\n",
    "MAX_TOTAL_REWARD = 500\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TAU = 0.1\n",
    "LAYER_1 = 128\n",
    "LAYER_2 = 64\n",
    "ALPHA = 0.0000001\n",
    "GAMMA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.buffer, size)\n",
    "        random_index_list = np.random.choice(np.arange(len(self.buffer)), size = size, replace = False)\n",
    "        sublist =  [self.buffer[index] for index in random_index_list]\n",
    "        states, actions, rewards, next_states, done = zip(*sublist)\n",
    "        return states, actions, rewards, next_states, done\n",
    "\n",
    "    def remember(self, experience):\n",
    "        self.buffer.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(LAYER_1, activation = 'relu', input_shape = (state_dim, ), kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(action_dim, activation = 'tanh', kernel_initializer='he_uniform'))\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss=self.newLoss)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model.predict(np.array(state))\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))\n",
    "\n",
    "    def train(self, state, errors, batch_size):\n",
    "        self.model.fit( state, errors, batch_size=batch_size, verbose = 0)\n",
    "\n",
    "    def newLoss(self, xPred,yPred):\n",
    "        return K.sum(-1*yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.first_input = Input((state_dim, ))\n",
    "        self.first_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.first_input)\n",
    "\n",
    "        self.second_input = Input((action_dim, ))\n",
    "        self.second_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.second_input)\n",
    "\n",
    "        self.merged = Concatenate(axis = 1)([self.first_dense, self.second_dense])\n",
    "        self.output_layer = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.merged)\n",
    "        self.output_layer_2 = Dense(1, kernel_initializer='he_uniform')(self.output_layer)\n",
    "        \n",
    "        self.model = Model(inputs=[self.first_input, self.second_input], outputs=self.output_layer_2)\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss='mse')\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.model.predict([state, action])\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, state_dim, action_dim, memory, load):\n",
    "        self.memory = memory\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(action_dim)\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.target_actor = Actor(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "        if load != 0:\n",
    "            self.load_models(load) #load the model\n",
    "\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "\n",
    "    def get_action(self, state, train):\n",
    "        if train:\n",
    "            action = self.actor.forward([state])\n",
    "            noise = np.float32(self.noise.sample())\n",
    "            return action + noise\n",
    "        action = self.actor.forward([state])\n",
    "        return action\n",
    "\n",
    "    def optimize(self):\n",
    "        state,action,reward,next_state, done = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        next_state = np.array(next_state)\n",
    "        done = 1-np.array(done)\n",
    "        \n",
    "        next_action = self.target_actor.forward(next_state)\n",
    "        target = reward[:,np.newaxis] + GAMMA* self.target_critic.forward(next_state, next_action)*done[:, np.newaxis]\n",
    "        self.critic.model.fit([state,action], target, batch_size=BATCH_SIZE, verbose = 0)\n",
    "\n",
    "        action = self.actor.forward(state)\n",
    "        self.actor.train(state, self.critic.forward(state, action), BATCH_SIZE)\n",
    "        \n",
    "        self.net_update(self.target_actor, self.actor, False)\n",
    "        self.net_update(self.target_critic, self.critic, False)\n",
    "\n",
    "    def net_update(self,target, source, hard):\n",
    "        degree = 1\n",
    "        if not hard: degree = TAU\n",
    "        target.update(source, degree)\n",
    "\n",
    "    def save_models(self, episode):\n",
    "        self.actor.model.save_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.save_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "\n",
    "    def load_models(self, episode):\n",
    "        self.actor.model.load_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.load_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "        print('Models loaded succesfully')\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, action_dim):\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = 0.15\n",
    "        self.sigma = 0.02\n",
    "        self.dx = np.zeros(self.action_dim)\n",
    "\n",
    "    def sample(self):\n",
    "        self.dx = self.dx + self.theta * (-self.dx) + self.sigma * np.random.randn(len(self.dx))\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\n",
      " Testing agent got a reward of : -394.498217489879\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "\n",
      " Testing agent got a reward of : -313.3611612615508\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "10\n",
      "\n",
      " Testing agent got a reward of : -488.86641009722774\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "15\n",
      "\n",
      " Testing agent got a reward of : -101.94041689570041\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "20\n",
      "\n",
      " Testing agent got a reward of : -496.53002905202777\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "25\n",
      "\n",
      " Testing agent got a reward of : -496.47988769405964\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "30\n",
      "\n",
      " Testing agent got a reward of : -554.5935981925503\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "35\n",
      "\n",
      " Testing agent got a reward of : -508.9797387023366\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "40\n",
      "\n",
      " Testing agent got a reward of : -123.44761323130274\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "45\n",
      "\n",
      " Testing agent got a reward of : -423.5723062207639\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "50\n",
      "\n",
      " Testing agent got a reward of : -711.3844235667899\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "55\n",
      "\n",
      " Testing agent got a reward of : -470.06306960248367\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "60\n",
      "\n",
      " Testing agent got a reward of : -319.9997380089023\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "65\n",
      "\n",
      " Testing agent got a reward of : -430.9250030079048\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "70\n",
      "\n",
      " Testing agent got a reward of : -345.0215711666841\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "75\n",
      "\n",
      " Testing agent got a reward of : -639.6208285833577\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "80\n",
      "\n",
      " Testing agent got a reward of : -548.1745720709348\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "85\n",
      "\n",
      " Testing agent got a reward of : -541.4350692614992\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "90\n",
      "\n",
      " Testing agent got a reward of : -362.01071875132504\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "95\n",
      "\n",
      " Testing agent got a reward of : -654.0179114626488\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "\n",
      " Testing agent got a reward of : -429.2024650766163\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "105\n",
      "\n",
      " Testing agent got a reward of : -474.97462877920566\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "110\n",
      "\n",
      " Testing agent got a reward of : -415.7393843370365\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "115\n",
      "\n",
      " Testing agent got a reward of : -119.70652540217593\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "120\n",
      "\n",
      " Testing agent got a reward of : -709.2464562044476\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "125\n",
      "\n",
      " Testing agent got a reward of : -433.07426100727497\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "130\n",
      "\n",
      " Testing agent got a reward of : -455.24279921850643\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "135\n",
      "\n",
      " Testing agent got a reward of : -478.56998057257\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "140\n",
      "\n",
      " Testing agent got a reward of : -729.2993832652511\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "145\n",
      "\n",
      " Testing agent got a reward of : -491.39093994757343\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "150\n",
      "\n",
      " Testing agent got a reward of : -411.5741034480614\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "155\n",
      "\n",
      " Testing agent got a reward of : -428.9241023924848\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "160\n",
      "\n",
      " Testing agent got a reward of : -639.1588041918495\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "165\n",
      "\n",
      " Testing agent got a reward of : -197.3060822985966\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "170\n",
      "\n",
      " Testing agent got a reward of : -615.7178991767249\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "175\n",
      "\n",
      " Testing agent got a reward of : -104.08775106625959\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "180\n",
      "\n",
      " Testing agent got a reward of : -517.7577657071896\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "185\n",
      "\n",
      " Testing agent got a reward of : -509.254397439752\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "190\n",
      "\n",
      " Testing agent got a reward of : -627.6987298619507\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "195\n",
      "\n",
      " Testing agent got a reward of : -592.283129251663\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "200\n",
      "\n",
      " Testing agent got a reward of : -585.1222560764456\n",
      "201\n",
      "202\n",
      "203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0menv_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mtest_interwal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mmax_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-fe71b109b94f>\u001b[0m in \u001b[0;36menv_run\u001b[1;34m(env, episode, trainer, memory, train)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ef97cb0554cd>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ce0addde0503>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def env_run(env, episode, trainer, memory, train):\n",
    "    state = env.reset()\n",
    "    epoch_reward = 0\n",
    "    print(episode)\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        if not train:\n",
    "            env.render()\n",
    "        action = trainer.get_action(state, train)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        epoch_reward +=reward\n",
    "        if train:\n",
    "            if done:\n",
    "                break\n",
    "            memory.remember((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            trainer.optimize()\n",
    "        else:\n",
    "            if done:\n",
    "                env.close()\n",
    "                print(\"\\n Testing agent got a reward of :\",epoch_reward)\n",
    "                break\n",
    "        state = next_state\n",
    "    gc.collect()\n",
    "    if episode%100 == 1:\n",
    "        trainer.save_models(episode)\n",
    "    return epoch_reward\n",
    "\n",
    "def prepopulate_memory(memory, env):\n",
    "    state = env.reset()\n",
    "    for _ in range(MAX_BUFFER):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.remember((state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "def main():\n",
    "    training = 1\n",
    "    test_interwal = 5\n",
    "    load = 0\n",
    "\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    memory = None\n",
    "\n",
    "    if training == 1:\n",
    "        memory = Memory(MAX_BUFFER)\n",
    "        prepopulate_memory(memory, env)\n",
    "    rewards = []\n",
    "    max_reward = 0\n",
    "\n",
    "    trainer = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0], memory, load)\n",
    "\n",
    "    for episode in np.arange(MAX_EPISODES):\n",
    "        if training == 1:\n",
    "            env_run(env, episode, trainer, memory, True)\n",
    "        if episode%test_interwal == 0:\n",
    "            max_reward += env_run(env, episode,trainer, None, False)\n",
    "            rewards.append(max_reward/((episode/test_interwal)+1))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
