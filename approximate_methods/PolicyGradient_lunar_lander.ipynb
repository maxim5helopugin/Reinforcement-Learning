{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "import gc\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Concatenate, Input, Dense\n",
    "from keras.layers import merge\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_EPISODES = 200\n",
    "MAX_STEPS = 10000\n",
    "MAX_BUFFER = 40000\n",
    "MAX_TOTAL_REWARD = 300\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TAU = 0.0001\n",
    "LAYER_1 = 128\n",
    "LAYER_2 = 64\n",
    "ALPHA = 0.002\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def sample(self, size):\n",
    "        batch = random.sample(self.buffer, size)\n",
    "        random_index_list = np.random.choice(np.arange(len(self.buffer)), size = size, replace = False)\n",
    "        sublist =  [self.buffer[index] for index in random_index_list]\n",
    "        states, actions, rewards, next_states = zip(*sublist)\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def remember(self, experience):\n",
    "        self.buffer.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(LAYER_1, activation = 'relu', input_shape = (state_dim, ), kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(action_dim, activation = 'linear', kernel_initializer='he_uniform'))\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss=self.newLoss)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model.predict(np.array(state))\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))\n",
    "\n",
    "    def train(self, state, errors, batch_size):\n",
    "        self.model.fit( state, errors, batch_size=batch_size, verbose = 0)\n",
    "\n",
    "    def newLoss(self, xPred,yPred):\n",
    "        return np.sum(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.first_input = Input((state_dim, ))\n",
    "        self.first_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.first_input)\n",
    "\n",
    "        self.second_input = Input((action_dim, ))\n",
    "        self.second_dense = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.second_input)\n",
    "\n",
    "        self.merged = Concatenate()([self.first_dense, self.second_dense])\n",
    "        self.output_layer = Dense(LAYER_2, activation = 'relu', kernel_initializer='he_uniform')(self.merged)\n",
    "        self.output_layer_2 = Dense(1, kernel_initializer='he_uniform')(self.output_layer)\n",
    "        \n",
    "        self.model = Model(inputs=[self.first_input, self.second_input], outputs=self.output_layer_2)\n",
    "        self.adam = Adam(lr=ALPHA)\n",
    "        self.model.compile(optimizer=self.adam, loss='mse')\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.model.predict([state, action])\n",
    "\n",
    "    def update(self, source, degree):\n",
    "        self.model.set_weights((np.array(source.model.get_weights())*degree)+(np.array(self.model.get_weights())*(1-degree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, state_dim, action_dim, memory, load):\n",
    "        self.memory = memory\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(action_dim)\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.target_actor = Actor(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "\n",
    "        if load != 0:\n",
    "            self.load_models(load) #load the model\n",
    "\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "\n",
    "    def get_action(self, state, train):\n",
    "        if train:\n",
    "            action = self.actor.forward([state])\n",
    "            noise = np.float32(self.noise.sample())\n",
    "            return action + noise\n",
    "        action = self.actor.forward([state])\n",
    "        return action\n",
    "\n",
    "    def optimize(self):\n",
    "        state,action,reward,next_state = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        state = np.array(state)\n",
    "        action = np.array(action)\n",
    "        reward = np.array(reward)\n",
    "        next_state = np.array(next_state)\n",
    "\n",
    "        next_action = self.target_actor.forward(next_state)\n",
    "\n",
    "        target = reward[:,np.newaxis] + GAMMA* self.target_critic.forward(next_state, next_action)\n",
    "        self.critic.model.fit([state,action], target, batch_size=BATCH_SIZE, verbose = 0)\n",
    "\n",
    "        action = self.actor.forward(state)\n",
    "        self.actor.train(state, self.critic.forward(state, action), BATCH_SIZE)\n",
    "        self.net_update(self.target_actor, self.actor, False)\n",
    "        self.net_update(self.target_critic, self.critic, False)\n",
    "\n",
    "    def net_update(self,target, source, hard):\n",
    "        degree = 1\n",
    "        if not hard: degree = TAU\n",
    "        target.update(source, degree)\n",
    "\n",
    "    def save_models(self, episode):\n",
    "        self.actor.model.save_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.save_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "\n",
    "    def load_models(self, episode):\n",
    "        self.actor.model.load_weights('Models/' + str(episode) + 'actor_weights.h5')\n",
    "        self.critic.model.load_weights('Models/' + str(episode) + 'critic_weights.h5')\n",
    "        self.net_update(self.target_actor, self.actor, True)\n",
    "        self.net_update(self.target_critic, self.critic, True)\n",
    "        print('Models loaded succesfully')\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, action_dim):\n",
    "        self.action_dim = action_dim\n",
    "        self.theta = 0.15\n",
    "        self.sigma = 0.2\n",
    "        self.dx = np.zeros(self.action_dim)\n",
    "\n",
    "    def sample(self):\n",
    "        self.dx = self.dx + self.theta * (-self.dx) + self.sigma * np.random.randn(len(self.dx))\n",
    "        return self.dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\n",
      " Testing agent got a reward of : -331.90048389958804\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "\n",
      " Testing agent got a reward of : -411.122448715369\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "10\n",
      "\n",
      " Testing agent got a reward of : -308.77591274040805\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "15\n",
      "\n",
      " Testing agent got a reward of : -589.9832708198111\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "20\n",
      "\n",
      " Testing agent got a reward of : -757.7523877404694\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "25\n",
      "\n",
      " Testing agent got a reward of : -815.5923609687721\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "30\n",
      "\n",
      " Testing agent got a reward of : -468.9086326959192\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "35\n",
      "\n",
      " Testing agent got a reward of : -646.9092733093082\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "40\n",
      "\n",
      " Testing agent got a reward of : -688.4848644763729\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "45\n",
      "\n",
      " Testing agent got a reward of : -809.2504229733386\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "50\n",
      "\n",
      " Testing agent got a reward of : -433.4999548161564\n",
      "51\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "def env_run(env, episode, trainer, memory, train):\n",
    "    state = env.reset()\n",
    "    epoch_reward = 0\n",
    "    print(episode)\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        if not train:\n",
    "            env.render()\n",
    "        action = trainer.get_action(state, train)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        epoch_reward +=reward\n",
    "        if train:\n",
    "            if done:\n",
    "                break\n",
    "            memory.remember((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            trainer.optimize()\n",
    "        else:\n",
    "            if done:\n",
    "                env.close()\n",
    "                print(\"\\n Testing agent got a reward of :\",epoch_reward)\n",
    "                break\n",
    "        state = next_state\n",
    "    gc.collect()\n",
    "    if episode%100 == 1:\n",
    "        trainer.save_models(episode)\n",
    "    return epoch_reward\n",
    "\n",
    "def prepopulate_memory(memory, env):\n",
    "    state = env.reset()\n",
    "    for _ in range(MAX_BUFFER):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.remember((state, action, reward, next_state))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "def main():\n",
    "\n",
    "    training = 1\n",
    "    test_interwal = 5\n",
    "    load = 0\n",
    "\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "    memory = None\n",
    "\n",
    "    if training == 1:\n",
    "        memory = Memory(MAX_BUFFER)\n",
    "        prepopulate_memory(memory, env)\n",
    "    rewards = []\n",
    "    max_reward = 0\n",
    "\n",
    "    trainer = ActorCritic(env.observation_space.shape[0], env.action_space.shape[0], memory, load)\n",
    "\n",
    "    for episode in np.arange(MAX_EPISODES):\n",
    "        if training == 1:\n",
    "            env_run(env, episode, trainer, memory, True)\n",
    "        if episode%test_interwal == 0:\n",
    "            max_reward += env_run(env, episode,trainer, None, False)\n",
    "            rewards.append(max_reward/((episode/test_interwal)+1))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
